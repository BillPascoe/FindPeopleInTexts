{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba0f207-59f2-4d47-bfd1-47ca7349c468",
   "metadata": {},
   "source": [
    "# Find People In Texts\n",
    "\n",
    "This code reads input texts and identifies people in them, outputting their names, the sentence their name appears in, a citation and URL if available as a CSV file. A CSV file can be opened in Excel for correcting and adding other data. \n",
    "\n",
    "These methods will save a great deal of time and make large scale research feasible when before it was not. None the less, these methods unavoidably have a high error rate and will require a substantial amount of human checking and manual correction.\n",
    "\n",
    "The input format is not strict markup, but is designed to be very quick and easy to use for a non IT person trawling full text OCR archives. Just copy and paste the citation and the text and seperate with a line of hashes.\n",
    "\n",
    "It will also attempt to identify dates from the citation to help piece together people's life course, or the order events they were involved in. It may pick up the 'accessed on' date or other date instead of the publication date. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7391ed-085c-49ad-b8bd-c247fdf64da2",
   "metadata": {},
   "source": [
    "## TO DO\n",
    "- file upload button (if you are using this online in Binder Hub, you can use Binder Hub to upload a file, and set the file name in settings below).\n",
    "- Optional Get locations.\n",
    "- Optional Geolocate locations.\n",
    "- LOD for link people and places.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26522d49-00db-4575-84e1-001d5be6174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confirm script is working by printing this sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537f3c9-b406-4300-baaf-a26fcaa92e11",
   "metadata": {},
   "source": [
    "## Inputs, options and settings\n",
    "\n",
    "### Input\n",
    "\n",
    "The expected input is a plain text file with a citation at the top, seperated from the text by a blank line. \n",
    "\n",
    "You can also process many small texts, such as news articles from Trove, by putting them in a single text file. \n",
    "\n",
    "Simply seperate each text with a line of hash tags, and put the citation at the start of each section, followed by a blank line and then the full text.\n",
    "\n",
    "An example text 'Example_Eureka.txt' has been provided. This example includes texts with few and with many names, and some with OCR errors, to illustrate the sorts of things you are likely to encounter. \n",
    "\n",
    "Eg:\n",
    "\n",
    "The Australian, p. 2. Retrieved June 9, 2025, from http://nla.gov.au/nla.news-article1\n",
    "\n",
    "This is a full news story about something or other on the night of the ...\n",
    "\n",
    "########################\n",
    "\n",
    "Sydney Morning Herald, p. 5. Retrieved June 9, 2025, from http://nla.gov.au/nla.news-article2\n",
    "\n",
    "Another full text article is here, according to eye witness accounts ...\n",
    "\n",
    "###############\n",
    "\n",
    "etc\n",
    "\n",
    "### Options\n",
    "\n",
    "show_highlighted_text\n",
    "Set show_highlighted_text to True if you would like to print the text to screen with all the identified people highlighted. This can be useful for checking results and making manual corrections in the output file. This also highlights many other 'entities' such as organisations, dates, etc, which are each colour coded (people are purple). All entities are included to help find people misidentified as places, or otherwise. Set to False, if you just want to process and output the text.\n",
    "\n",
    "This will highlight not only people but other types of things that it identifies, such as places, dates, organisations, etc. It does get a lot wrong, but it still useful.\n",
    "\n",
    "results_to_screen\n",
    "Set to true if you want to see the results listed to this screen. \n",
    "\n",
    "write_to_file\n",
    "Set to true if you want to output the results to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713be93b-8fd4-4c35-9b26-dffe2d53f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"Example_Eureka.txt\"\n",
    "show_highlighted_text = True\n",
    "results_to_screen = True\n",
    "write_to_file = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49942d62-ad56-43a9-bd12-479f4527c4f7",
   "metadata": {},
   "source": [
    "## Utility and helper functions for getting IDs and dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a1db7-848d-48f9-8ca8-33e083cfd832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict\n",
    "\n",
    "# Lines of only hashes split blocks; blank line splits citation vs body\n",
    "_RE_BLOCK_SPLIT = re.compile(r'^\\s*#+\\s*$', re.MULTILINE)\n",
    "_RE_BLANKLINE   = re.compile(r'\\n\\s*\\n', re.MULTILINE)\n",
    "_RE_URL         = re.compile(r'https?://[^\\s)]+', re.IGNORECASE)\n",
    "\n",
    "# helper function for getting a unique identifier, either URL, DOI or ISBN or creating one Short-hash URN\n",
    "import re, unicodedata, hashlib\n",
    "\n",
    "RE_URL = re.compile(r'https?://[^\\s)]+', re.I)\n",
    "RE_DOI = re.compile(r'10\\.\\d{4,9}/\\S+', re.I)\n",
    "RE_ISBN = re.compile(r'\\bISBN(?:-1[03])?:?\\s*([0-9Xx][0-9Xx\\-\\s–]{8,})\\b')\n",
    "\n",
    "def _norm_for_hash(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    return re.sub(r'\\s+', ' ', s).strip().lower()\n",
    "\n",
    "def _strip_trailing_punct(s: str) -> str:\n",
    "    return s.rstrip(').,;]»”')\n",
    "\n",
    "def _clean_isbn(raw: str) -> str:\n",
    "    digits = re.sub(r'[\\s\\-\\u2013]', '', raw).upper()  # remove spaces/hyphens/en-dash\n",
    "    return digits if len(digits) in (10, 13) else ''\n",
    "\n",
    "def make_identifier(citation: str) -> str:\n",
    "    \"\"\"Return a stable identifier: URL, DOI URL, ISBN URN, or short-hash URN.\"\"\"\n",
    "    if not citation:\n",
    "        return \"urn:source:000000000000\"\n",
    "\n",
    "    # URL\n",
    "    m = RE_URL.search(citation)\n",
    "    if m:\n",
    "        return _strip_trailing_punct(m.group(0))\n",
    "\n",
    "    # DOI\n",
    "    m = RE_DOI.search(citation)\n",
    "    if m:\n",
    "        doi = _strip_trailing_punct(m.group(0))\n",
    "        return f\"https://doi.org/{doi}\"\n",
    "\n",
    "    # ISBN (expects 'ISBN' present; avoids false positives)\n",
    "    m = RE_ISBN.search(citation)\n",
    "    if m:\n",
    "        isbn = _clean_isbn(m.group(1))\n",
    "        if isbn:\n",
    "            return f\"urn:isbn:{isbn}\"\n",
    "\n",
    "    # Fallback: deterministic short hash of normalized citation\n",
    "    short = hashlib.sha1(_norm_for_hash(citation).encode(\"utf-8\")).hexdigest()[:12]\n",
    "    return f\"urn:source:{short}\"\n",
    "\n",
    "# DETECT DATE\n",
    "from typing import Tuple\n",
    "\n",
    "# Map month names -> MM\n",
    "_MONTH = {\n",
    "    \"jan\": \"01\",\"january\": \"01\",\"feb\": \"02\",\"february\": \"02\",\"mar\": \"03\",\"march\": \"03\",\n",
    "    \"apr\": \"04\",\"april\": \"04\",\"may\": \"05\",\"jun\": \"06\",\"june\": \"06\",\"jul\": \"07\",\"july\": \"07\",\n",
    "    \"aug\": \"08\",\"august\": \"08\",\"sep\": \"09\",\"sept\": \"09\",\"september\": \"09\",\"oct\": \"10\",\"october\": \"10\",\n",
    "    \"nov\": \"11\",\"november\": \"11\",\"dec\": \"12\",\"december\": \"12\"\n",
    "}\n",
    "\n",
    "def get_date(citation: str):\n",
    "    # 0) ignore retrieval/access dates\n",
    "    cut = re.search(r'\\b(retrieved|accessed|viewed)\\b', citation, re.I)\n",
    "    head = citation[:cut.start()] if cut else citation\n",
    "\n",
    "    # 1) (1839, August 29)\n",
    "    m = re.search(r'\\(\\s*(\\d{4})\\s*,\\s*([A-Za-z]+)\\s+(\\d{1,2})\\s*\\)', head)\n",
    "    if m and _MONTH.get(m.group(2).lower()):\n",
    "        y, mon, d = m.group(1), m.group(2), m.group(3)\n",
    "        return (m.group(0), f\"{y}-{_MONTH[mon.lower()]}-{int(d):02d}\")\n",
    "\n",
    "    # 2) 1839, August 29\n",
    "    m = re.search(r'\\b(\\d{4})\\s*,\\s*([A-Za-z]+)\\s+(\\d{1,2})\\b', head)\n",
    "    if m and _MONTH.get(m.group(2).lower()):\n",
    "        y, mon, d = m.groups()\n",
    "        return (m.group(0), f\"{y}-{_MONTH[mon.lower()]}-{int(d):02d}\")\n",
    "\n",
    "    # 3) 29 August 1839\n",
    "    m = re.search(r'\\b(\\d{1,2})\\s+([A-Za-z]+)\\s+(\\d{4})\\b', head)\n",
    "    if m and _MONTH.get(m.group(2).lower()):\n",
    "        d, mon, y = m.groups()\n",
    "        return (m.group(0), f\"{y}-{_MONTH[mon.lower()]}-{int(d):02d}\")\n",
    "\n",
    "    # 4) August 29, 1839\n",
    "    m = re.search(r'\\b([A-Za-z]+)\\s+(\\d{1,2}),?\\s+(\\d{4})\\b', head)\n",
    "    if m and _MONTH.get(m.group(1).lower()):\n",
    "        mon, d, y = m.groups()\n",
    "        return (m.group(0), f\"{y}-{_MONTH[mon.lower()]}-{int(d):02d}\")\n",
    "\n",
    "    # 5) Month Year\n",
    "    m = re.search(r'\\b([A-Za-z]+)\\s+(\\d{4})\\b', head)\n",
    "    if m and _MONTH.get(m.group(1).lower()):\n",
    "        mon, y = m.groups()\n",
    "        return (m.group(0), f\"{y}-{_MONTH[mon.lower()]}\")\n",
    "\n",
    "    # 6) Year range\n",
    "    m = re.search(r'\\b(c\\.|ca\\.|circa\\s*)?(\\d{4})\\s*[–-]\\s*(\\d{4})\\b', head, re.I)\n",
    "    if m:\n",
    "        return (m.group(0).strip(), \"\")\n",
    "\n",
    "    # 7) Single year\n",
    "    m = re.search(r'\\b(?:c\\.|ca\\.|circa\\s*)?(\\d{4})(?!\\d)\\b', head, re.I)\n",
    "    if m:\n",
    "        y = m.group(1)\n",
    "        return (m.group(0).strip(), y)\n",
    "\n",
    "    return (\"\", \"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d1536-7a12-41e8-baec-6e75b09f89c4",
   "metadata": {},
   "source": [
    "## Split input file into citations, texts and URL\n",
    "\n",
    "Function for splitting the input text into several texts (assuming they are seperated by a line of hashtags: ########), \n",
    "extracting the citation and the URL and returning a data structure with URL as the key, \n",
    "and containing the citation and text of the article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751f9cda-f1a9-40fa-8b09-e39b1a393c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_blocks(big_text: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Input text contains blocks separated by a line of hashes (#####...).\n",
    "    Each block: citation at top, then a blank line, then the text.\n",
    "    Returns { url: {\"citation\": citation, \"text\": body} }.\n",
    "    Blocks without a URL in the citation are skipped.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Dict[str, str]] = {}\n",
    "    for chunk in _RE_BLOCK_SPLIT.split(big_text):\n",
    "        block = chunk.strip()\n",
    "        if not block:\n",
    "            continue\n",
    "\n",
    "        parts = _RE_BLANKLINE.split(block, maxsplit=1)\n",
    "        citation = parts[0].strip()\n",
    "        body = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "        # get date\n",
    "        rawdate, iso_date = get_date(citation)\n",
    "        date = \"-\".join((iso_date.split(\"-\") + [\"01\",\"01\"])[:3]) if iso_date else \"\"\n",
    "        \n",
    "        source_id = make_identifier(citation)\n",
    "        out[source_id] = {\"citation\": citation, \"text\": body, \"date\": date}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5757da-8538-4076-aef5-4da8e5010ad5",
   "metadata": {},
   "source": [
    "## Find people in each text\n",
    "Function for identifying people in a text, and displaying them using Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59a44ff-3721-4af7-a5c8-54f651ae9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# load once\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def find_people(text: str, show_displacy: bool = True):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    people, seen = [], set()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name = \" \".join(ent.text.split())\n",
    "            sent = ent.sent.text.strip()\n",
    "            key = (name, sent)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                people.append(key)\n",
    "\n",
    "    if show_displacy:\n",
    "        displacy.render(doc, style=\"ent\")\n",
    "\n",
    "    return people\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1fdd6-569c-4e49-8c7c-28a65ce5a7c6",
   "metadata": {},
   "source": [
    "## Call the functions to process each text and find people\n",
    "Read text from file and split into articles, and identify citation and URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e6a06d-202c-4614-a2f5-1c35486825cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
    "     intext = f.read()\n",
    "\n",
    "text_data = split_text_blocks(intext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c89c9b-4936-4974-8299-692ddaf46368",
   "metadata": {},
   "source": [
    "Find the people in each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038bdfa-fb9b-41af-9b3e-52491ab39438",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for url, item in text_data.items():\n",
    "    # print(url, item[\"citation\"])\n",
    "    print(\"Finding people...\")\n",
    "    print(item[\"citation\"])\n",
    "    people = find_people(item[\"text\"], show_highlighted_text)\n",
    "    item[\"people\"] = people\n",
    "    # report to screen\n",
    "    #for name, sentence in people:\n",
    "    #    print(f\"{name} → {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cb9dc-bfd0-424a-b10b-2d98ca500239",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (results_to_screen):\n",
    "    for url, item in text_data.items():\n",
    "        print(\"Citation: \" + item.get(\"citation\", \"\"))\n",
    "        print(\"Date: \" + item.get(\"date\", \"\"))\n",
    "        print()\n",
    "        for p in item.get(\"people\", []):\n",
    "            if isinstance(p, dict):\n",
    "                print(\"Name: \" + p.get(\"name\", \"\"))\n",
    "                print(\"Sentence: \" + p.get(\"sentence\", \"\"))\n",
    "                print()\n",
    "            else:  # assume tuple/list\n",
    "                name, sentence = (p + (\"\",))[:2]\n",
    "                print(\"Name: \" + name)\n",
    "                print(\"Sentence: \" + sentence)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933faf13-e970-40b8-b9f3-6f4e001ed875",
   "metadata": {},
   "source": [
    "## Output file\n",
    "\n",
    "Output result to CSV. CSV files can be opened in Excel and corrected, and other information added, such as whether the person is colonist, Aboriginal, Torres Strait Islander, their country, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42ca5c-7383-4647-8f98-feff25a33ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (write_to_file):\n",
    "\n",
    "    import csv\n",
    "    \n",
    "    def export_people_csv(text_data, csv_path):\n",
    "        \"\"\"\n",
    "        text_data: {\n",
    "          url: {\"citation\": str, \"fulltext\": str, \"people\": [\n",
    "                  # either:\n",
    "                  (name, sentence)  OR  {\"name\": name, \"sentence\": sentence}\n",
    "          ]}\n",
    "        }\n",
    "        Writes CSV with columns: person, sentence, citation, URL\n",
    "        \"\"\"\n",
    "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow([\"person\", \"sentence\", \"date\", \"citation\", \"URL\"])\n",
    "            for url, item in text_data.items():\n",
    "                citation = item.get(\"citation\", \"\")\n",
    "                date = item.get(\"date\", \"\")\n",
    "                for p in item.get(\"people\", []):\n",
    "                    if isinstance(p, dict):\n",
    "                        name = p.get(\"name\", \"\")\n",
    "                        sentence = p.get(\"sentence\", \"\")\n",
    "                    else:  # assume tuple/list\n",
    "                        name, sentence = (p + (\"\",))[:2]\n",
    "                    w.writerow([name, sentence, date, citation, url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596cde26-7be9-4a6a-9863-06ffffb12ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_people_csv(text_data, \"people.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669678c8-4827-4987-998d-12e24309639b",
   "metadata": {},
   "source": [
    "## Data cleaning post processing\n",
    "\n",
    "Don't be worried if there are hundreds of rows in the results. Many of them are false positives and can quickly be deleted, leaving you with a manageable number to work through.\n",
    "\n",
    "You will probably want to take the following manual data cleaning steps:\n",
    "\n",
    "- Remove false positives from glitchy data, and things which aren't people (eg: places are sometimes misidentified as people).\n",
    "- Remove people from news not relevant to the conflict (eg: sometimes an article may summarise many disconnected events, such as Jackey raiding a hut as well as Mr. Weber buying a horse. Remove Mr Weber.)\n",
    "- Correct spelling errors.\n",
    "- Check each record against the source that it is a properly identify person and should be included.\n",
    "- Check the highlighted output in the script for anyone not identified and manually add to the spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447e9387-c3cb-4177-ad05-9587356bdea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
